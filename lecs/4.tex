\chapter{Ensemble Methods}

{\sf The ensemble method is just uniting the outputs of several classifiers into one. So let's say we have three classifiers (you are already familiar with KNN and Decision Trees and will become acquainted with Support Vector Machines in a couple of lectures). They have different outputs for different classes in your space. You can merge those predictions into one and have a unified answer. The simple way to do it is called voting.}

\section{Voting}

 Voting can be hard or soft.

 Hard voting is when we count the number of votes to produce an answer. For example, if I have five trees and most of them get class A as an answer, my answer will be A. If we have an even number of trees and half of them say <<Сlass A is the answer>> and others say <<Class B is the answer>>, we choose the answer by taking into account whether we want the Higher Precision or Higher Recall, or choose a class in alphabetical order (this is the way it is done in the scikit-learn package). We can avoid this issue by having an odd number of classifiers.

 Soft voting takes into account how sure the classifier in its decision. For example, two classifiers get the pair (0.1, 0.9) as an answer (0.1 is the probability of class A, 0.9 is the probability of class B) and three classifiers say <<Not sure>> (pair (0.51, 0.49)). The hard voting will give us class A as an answer (because the last three classifiers get class A). However, the last three classifiers don't give us a decisive answer, because (0.51, 0.49) is so close to the neutral answer (0.5, 0.5) In soft voting, you could multiply or add up the classifiers' probabilities for each class and then choose an answer.

\section{Random Forest}

One of the best algorithms which utilizes the voting system is called Random Forest. Just from the name, you can guess that the classifiers are trees. The sampling strategy makes it random. We have our dataset but not all of the data points. So we simulate sampling from the general data by sampling from the dataset and then build a tree for each sample. This will help us find a noise. [Забегая вперед: мы строим каждое дерево не на всем объеме данных, а на какой-то его части. Эту часть мы будем выбирать исходя из некоторых весов точек. Сами веса при этом меняются после каждого построения дерева. Еще можно брать все точки, а веса как-то учитывать при построении деревьев.]\\
The greatest thing about Random Forest is that the more trees you have the better it is: there is no overfitting over the number of trees. Out of the sampling strategies we mostly use the combination of bagging and random subspaces. Let's just go through all of them.

\subsubsection*{Pasting}

Pasting is a simple sampling: let's take out of $N$ points $K$ random with no repeats.

\subsubsection*{Bagging (bootstrap aggregating)}

Bagging (aka bootstrap aggregating) is a statistical term for creating a dataset with the same size as the sample dataset but with repeating elements. Let's say we have a dataset with some features and two classes of the points from the dataset, as well as weights for all points. When we calculate the final error we multiply the points' errors by their weight. The thing we are going to do the most today is manipulating that weight. \\
Let's say we want to choose points with repeats. If the sampling weights are the same, you can just take a random point. If the sampling weights are different and you want to sample with those weights, you can do the thing we did when we thought about how to sample in the k-means$++$ algorithm. <...> \\
So you can use weights in your classifier. For example, when you build a tree you could use the Gini formula ($\sum_{y\in Y}\frac{1}{|X|}\cdot|x: y(x) =y|\cdot|x: y(x) \ne y|$), but instead of counting the number of points you can add up their weights. 
There are different strategies for sampling; you can pick points according to their probability, or you can use probability as a weight function in the impurity calculation. Additionally, instead of taking random samples of $N$, it is possible to assign random numbers as weights. The best approximation for bootstrapping is a Poisson distribution, so you can use random numbers with the Poisson distribution as weights in your impurity calculation.

\subsubsection*{Random subspaces}

The random subspaces is a way to try to randomize the decision process and have a good workaround for noise. We don't take random samples, but rather we take random features. Let's just use some subspace of features and random patches, so we use both random sample and random features. Now you can naively calculate what is called feature importance: if you want to know how important your feature is to the algorithm, you can build many trees and you can just calculate the number of trees that feature participates in. 

\subsubsection*{Extremely randomized trees}

Extremely randomized trees: instead of going over all the thresholds, over all the features, you can just say <<Ok, let's pick a hundred random thresholds and then pick the best>>. It doesn't work by itself, so if you want to construct one tree you will construct a really bad tree. However if you want to construct a forest, it's not a bad idea. You can construct a good forest with extremly randomized trees.\\
{\it <Some talk about how random forests are good.>}\\
{\it <A meme slide about bootstraps and Munchausen...>}

\section{Boosting Algorithms}

Random forests construct each tree basically independently, which is good because we can parellelize them. However, let's say we construct our first tree and it gives us a realy good answer with only a couple of problematic points. In that case we would not want the next tree to be random, we would rather want the next tree to take into account the results of the first tree. The algorithms which do that are called Boosting Algorithms. It is important to know that we don't actually need to build a deep tree. So in a random forest we prefer to use trees with depth 5 or 6, and in adaptive boosting we use stumps (trees with only two leaves). In gradient boosting we use trees with depth 3 or 4. 

\subsubsection*{Adaptive Boosting}

So we have our dataset and the first thing we do is we construct the first classifier that gives us the answer. <...>\\
At the first step we do the weights are uniform, so every point has the weight $D_1(i)=\frac{1}{N}$. Then we build a tree to minimize the error $E_t = \sum_{i=1}^N D_t(i) \cdot E(h_t(x_i), y_i)$ [в простом варианте $E(h_t(x),y) = 1$, если не угадали класс и $0$, если угадали]. We select the best threshold out of all the features and then assign the weight $\alpha_t=\frac{1}{2}\ln(\frac{1-E_t}{E_t})$ to that tree. {\it <A little description of this function and how it helps us>} Now you need to recalculate the weights:
$$\begin{cases}
D_{t+1}(i) = D_t(i)e^{\alpha_t}&\text{for incorrectly classified points,}\\
D_{t+1}(i)=D_t(i)e^{-\alpha_t}&\text{for correctly classified points.}
\end{cases}$$
Now you have the new sampling weights and you need to repeat the process, creating a new tree. You should stop if you don't have any improvement in your impurity after taking the weights into account. The final hypothesis suggests the *some number* (hard voting) or the *some number with coefficients* (soft voting).\\
{\it <A slide with an example>}

\subsubsection*{Gradient Boosting}

When you want to get the best answer for your table data (not structured data like sounds, text, pictures etc) the best thing you can do is gradient boosting.
\begin{displayquote}
  {\sf \glqq I think everyone sucks at explaining gradient boosting algorithms. Especially the authors of gradient boosting algorithms.\grqq
  \begin{flushright}
    A.A. Shpilman
  \end{flushright}}
\end{displayquote}
It is one of the trickier algorithms to understand, but it isn't as hard to implement. It works, it has math behind it, but it's hard to understand why it works. There will be two explanations: an easy one and a hard one.
\begin{enumerate}
  \item[Easy:] This is the general formula for the gradient boosting:
  $$H_{t+1}(x)=H_t(x)+h_{t+1}(x)\to y \Rightarrow h_{t+1}(x)\to y - H_t(x)$$
  It looks like gradient descent. {\it <A little meeting with the gradient descent>} Again, we build trees one by one, but we don't use stumps anymore. We use trees with depth 3 (or 4, or 5, smth like that) and we will use regression, not classification. So your tree predicts the numbers written in the leaves [$H_t(x)$ -- число, записанное в листе, в котором <<лежит>> точка $x$, в дереве $t$]. How do we get the number in a leaf? We find the average of all points that end up in this leaf [под суммированием точек подразумевается суммирование чисел, которыми мы обозначили классы точек, например для бинарной классификации это 1 и -1]. {\it <A slide with an example>} To get the next tree we need to subtract the number of each leaf of the previous tree from the points in this leaf and then we have the new y's [$h_{t+1}(x)$, новые классы точек]. <...> And then at some point you stop. That will give us the gradient booster: you take into account every new classifier, try to approximate the residual error and then you build the answer. We notice right away that we need to have a learning rate [learning rate -- это коэффициент $\alpha$ в выражении $H_{t+1}=H_t(x)+\alpha h_{t+1}(x)$]. So we still fit the tree to the residual but we add it with some learning rate $\alpha$ which is usually around 0.01--0.1. Why does it work better? No one knows, this is a purely experimental result. {\it <Some intuition about this experimental result>} Additionally, gradient boosting usually starts with the first tree having only one leaf with the average (you can also subtract the average right away, centering the data). So again, the error for gradient boosting is the mean square error: $$MSE = \frac{1}{N}\sum(y-H_t(x))^2$$ <...> So we build the tree to minimize that error and for each leaf, we define the weight as the mean of all the values in that leaf. It's simple. It works. Regression is very simple in terms of gradient boosting.\\
  {\it <Some talk about libraries. I'm not motivated enough to process that speech.>}
  \item[Hard:] What to do when it is not a simple regression, and we actually want to add some regularisation as well. This is where the complicated explanation comes in. So again:
  $$H_t=H_{t-1}(x)+h_t=\sum\limits_{j=1}^t h_j(x)$$
  In every step we want to calculate $h_t(x)$ and minimize the error $E_t$:
  $$E_t=\sum\limits_{i=1}^{N}L(H_t(x_i),y_i) + \sum\limits_{j=1}^t \Omega(h_j)$$
  In our regression example the function $L$ is just a MSE. Also we have some regularization term $\sum_{j=1}^t \Omega(h_j)$. $\Omega(h_t)$ is usually just the number of leaves of a tree $t$.
  $$E_t=\ldots=\sum\limits_{i=1}^{N}L(H_{t-1}(x_i) + h(x_i),y_i) + \sum\limits_{j=1}^{t-1} \Omega(h_j)+\Omega(h_t)=$$
  $$=\sum\limits_{i=1}^N\left(2(H_{t-1}(x_i)-y_i)h_t(x_i)+(h_t(x_i))^2\right)+\Omega(h_t)+const$$
  In this formula we already know everything except $h_t$ and $\Omega(h_t$ is what we are going to find in some steps in the gradient boosting process.\\
  In the general case $L$ is some differentiable function and you have:
  $$E_t=\sum\limits_{i=1}^N\left(L(H_{t-1}(x_i), y_i)+u_ih_t(x_i)+\frac{1}{2}v_i(h_t(x_i))^2\right)+\Omega(h_t)+const$$
  where
  $$u_i=\partial_{H_{t-1}(x_i)}\left(L(H_{t-1}(x_i), y_i)\right)$$
  $$v_i=\partial_{H_{t-1}(x_i)}^2\left(L(H_{t-1}(x_i), y_i)\right)$$
  In the regression case: $u_i=2(H_{t-1}(x_i)-y_i)$, $v_i=2$. \\
  So as long as your error function is differentiable you can calculate the function which you are trying to approximate (like this: $L(H_{t-1}(x_i), y_i)+u_ih_t(x_i)+\frac{1}{2}v_i(h_t(x_i))^2$; you want to minimize $\sum_{i=1}^N \left(u_ih_t(x_i)+\frac{1}{2}v_i(h_t(x_i))^2\right) + \Omega(h_t)$).\\
  The regularisation term $\Omega(f)$ in the case of extreme gradient boosting is the sum of all the leaves [слагаемое $\gamma M$, где $\gamma$ какая-то константа] plus sum of all the weights [$w_j$ -- число в листе $j$]:
  $$\Omega(f)=\gamma M+\frac{1}{2}\lambda\sum\limits_{j=1}^{M}w_j^2$$
  $$E_r=\sum\limits_{i=1}^{N}\left(u_iw_{q(x_i)}+\frac{1}{2}v_i(w_{q(x_i)})^2\right)+\gamma M + \frac{1}{2}\lambda\sum\limits_{j=1}^{M}w_j^2$$
  where $q(x_i)$ is the leaf of $x_i$.\\
  Then you can group that formula by leaves:
  $$E_t=\sum\limits_{j=1}^{M}\left(\sum\limits_{q(x_i)=j}u_iw_j+\frac{1}{2}\left(\sum\limits_{q(x_i)=j}v_i+\lambda\right)w_j^2\right)+\gamma M=\sum\limits_{j=1}^M\left(U_jw_j+\frac{1}{2}(V_j+\lambda)w_j^2\right)+\gamma M$$
  where $U_j=\sum\limits_{q(x_i)=j}u_i$ and $V_j=\sum\limits_{q(x_i)=j}v_i$ [$u_i$ и $v_i$ -- это те производные, что мы считали выше].\\
  So you have a function $E_t$ to minimize and this is the solution:
  $$w_j^{opt}=-\frac{U_j}{V_j+\lambda},\ E_t^{opt}=\gamma M-\frac{1}{2}\sum\limits_{j=1}^M\frac{U_j^2}{V_j+\lambda}$$
  This solution gives us the optimal weights to minimize our error when constructing a tree. The Gain corresponding to that error is:
  $$Gain = \frac{1}{2}\left[\frac{U_{left}^2}{V_{left}+\lambda}+\frac{U_{right}^2}{V_{right}+\lambda}-\frac{(U_{left}+U_{right})^2}{V_{left}+V_{right}+\lambda}\right]-\gamma$$
  So it is how you pick the threshold. The gradient boosting still goes over all thresholds and picks the best. But now you just use this Gain formula. <...>
\end{enumerate}
The thing you need to understand is we didn't actually change anything between constructing the trees in the gradient boosting (but we can if we want to make some points more important than the others).\\
{\it <Smth about CatBoost>}\\
And the one important thing is that the learning rate ($\alpha$ in $H_{t+1}=H_t(x)+\alpha h_{t+1}(x)$) scales the answer of the tree.