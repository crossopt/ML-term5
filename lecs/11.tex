\chapter{Global and Local Search}

{\sf Let's imagine we want to solve some kind of Traveling Salesman Problem (you have points that you need to travel to and you want to find the optimal way). How would we solve it with our methods? Well, we need to use the search methods. If we have a differentiable loss function we can use gradient descent, but if the space is very strange and we don't know how to calculate anything but we want to optimize it, we use global and local search. The global search does not analyze a local neighborhood of a point. The local search analyzes the local neighborhood of a point and goes in an optimal way.}

\section{Cross-Entropy Method}

In the cross-entropy (global search method) we are going to find a distribution that produces points near the optimum:
\begin{enumerate}
	\item At step $t=1$ we choose random parameters $\theta_0$ for a distribution.
	\item We generate a sample $x_1,\ldots,x_N$ from distribution $p(x|\theta_{t-1})$.
	\item We get the best $k$ points from $x_1,\ldots,x_N$ and find the next distribution using those points. So we optimize the next set of parameters based on the folowing criteria:
	$$\theta_t=\arg\min\limits_{u}\frac{1}{k}\sum\limits_{x_i\in \text{best }k}-y_i\cdot\frac{p(x_i|u)}{p(x_i|\theta_{t-1})}\log p(x_i| \theta_{t-1})$$
\end{enumerate}

\section{Hill Climb}

Hill climb is a local search method. It is called hill climb because it is sort of simulating climbing a hill at night: imagine you want to reach the top of the hill but you can't see it. It is similar to gradient ascent algorithm: you look around yourself and go in the direction where the hill rises. But the problem is that a local maximum may not be a global maximum, and when you reach the local maximum you will stop. Here some ways to correct that:
\begin{enumerate}[label=$\bullet$]
	\item {\sc Stochastic hill climbing.} Make steps with probability proportional to increase the metric (for example, with softmax).
	\item {\sc Taboo search.} We do not return to visited states.
	\item {\sc Particle swarm optimization.} We have multiple climbers that pass information: each step climber chooses a direction between its personal best position and the global best position.
	\item {\sc Simulated annealing.} We define direction by the softmax with the temperature:
	$$P(s_i)=\frac{e^{\Delta E(s_i)/T}}{\sum\limits_{j} e^{\Delta E(s_j)/T}}$$
	where $\Delta E(s_i)$ is the change of our function in the state $s_i$, $T$ is the temperature parameter. A high temperature makes probabilities $P(s_i)$ even and a lower temperature makes one of the probabilities prevalent.
\end{enumerate}

\subsubsection*{Quantum annealing}

\href{https://docs.dwavesys.com/docs/latest/c_gs_2.html}{Quantum computers find function optimum very fast}

\section{Genetic Algorithm}

Genetic algorithm works as evolution works: we select the best individuals and exchange <<genetic>> information by creating a new individual with some characteristics of its parents by concatenating some parts of them (plus adding some mutations). Then we replace the worst individual with the new one. The problem of this kind of algorithm is that it works very slowly. There is an important statement what is called <<no free lunch theorem>>.

\subsubsection*{No free lunch theorem}

Let's say we have two optimization algorithms $A_1$, $A_2$ and some sequence of values $d_m^y=\{y_1,\ldots,y_m\}$ of some function $f$. So the sum of probabilities to produce $d_m^y$ over all posible tasks $f$ is the same for $A_1$ and $A_2$:
$$\sum\limits_{f}P(d_m^y|f,m,A_1)=\sum\limits_{f}P(d_m^y|f,m,A_2)$$
It means that if one algorithm is the best in one task then another algorithm will be better in another.\\
One interpretation of this theorem is if we find the best algorithm $A$ for solving task $f$, then probably for a similar task $g$ algorithm $A$ will be also the best.