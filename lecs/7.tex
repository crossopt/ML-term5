\chapter{Deep Learning for Images}

{\sf Let's imagine we have a fully connected network for image recognition. The problem is, the bigger network we get the more connections the network has (ie the more chance of overfitting the network has, the more computing power needs for training etc). The main idea of deep learning is imposing a structure on that network. Rather than having a fully connected network we can have a structured network with a smaller number of weights, but more effective for solving some specific task.}

\section{Convolution}

Deep leaning was inspired by biology. In the early 1950's professors David Hubel and Torsten Wiesel made \href{https://youtu.be/IOHayh06LJ4}{a series of experiments} on a cat's brain. They tried to answer how cats see the world around them, how their visual cortex works. The experiments were putting a cat in some locking mechanism, closing one eye of the cat. Then they drill a little hole in its head and put an electrode at different place of the visual cortex to see how specific neurons reacts to specific visual stimulus. The cat saw some lines. So they found out that some neuron best detects lines with one angle, while some others detect lines with another angle [\href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363130/pdf/jphysiol01298-0128.pdf}{ссылка} на статью для интересующихся].

\subsubsection*{Detecting lines}

So if a cat sees with lines let's ask our neural network to see with lines. The detection of lines at various shapes was already known at that point and it was done in convolutions. The convolution is just pointwise multiplication and then addition. You can see [pic. 7.1] the green tensor which is our input image, the yellow part of it is a convolution filter and the red numbers are the values of this filter. What we do is we multiply each pixel value with the red number and then we sum them up.\\
\begin{figure}[h]
  \centering
  \begin{tabular}{cccc}
    \includegraphics[width=0.2\linewidth]{7a.png} &
    \includegraphics[width=0.2\linewidth]{7b.png} &
    \includegraphics[width=0.2\linewidth]{7c.png} &
    \includegraphics[width=0.2\linewidth]{7d.png} \\
    $2\cdot1+3\cdot1=5$ & $6\cdot1+1\cdot1=7$ & $4\cdot1+2\cdot1=6$ & $3\cdot1+5\cdot1=8$ \\
    & & & \\
    \multicolumn{4}{c}{(7.1) Convolution filter}
  \end{tabular}
\end{figure}
[В примере выше изображение было черно-белым: оно состояло из одного канала. Если же каналов больше, например, 3, то наш фильтр будет состоять уже из 3 слоев, необязательно одинаковых. Однако после прохода таким фильтром у изображения все равно останется один канал. Далее при указании размера фильтра я буду указывать размер одного слоя.]\\
So the parameters which we are trying to optimize are the kernels (size of the filter) and strides (how large is the shift of the filter which we use). You can see here [pic. 7.2] how various convolutions detect various features of the image:\\
\begin{figure}[h]
  \centering
  \begin{tabular}{ccc}
    $ \begin{pmatrix}
      0 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 0 \\
    \end{pmatrix} $ &
    $ \begin{pmatrix}
      1 & 0 & -1 \\
      0 & 0 & 0 \\
      -1 & 0 & 1 \\
    \end{pmatrix} $ &
    $ \begin{pmatrix}
      -1 & -1 & -1 \\
      -1 & 8 & -1 \\
      -1 & -1 & -1 \\
    \end{pmatrix} $ \\
    & & \\
    \includegraphics[width=0.2\linewidth]{7e.png} &
    \includegraphics[width=0.2\linewidth]{7f.png} &
    \includegraphics[width=0.2\linewidth]{7g.png} \\
    & & \\
    \multicolumn{3}{c}{(7.2) After applying some convolutions}
  \end{tabular}
\end{figure}

\subsubsection*{Convolution layers}

If you have simple shape detecting then you can have more complex shape detecting. You can get complex shape detecting by applying convolution to the features you get after you apply convolution for the first time. And so on. So now we have several convolution levels. What is important to understand that there still are neurons, there is still the multiplication of perceptron input by some weight. So all the math is still the same. However, when we back propagate the gradient we sum not over all neurons but over specific neurons, because all neurons in one convolution layer have the same weight.\\
Now let's imagine you have a typical image with size $224\times224\times3$ (3 because of color channels). It is called 2D convolution because it doesn't scan in depth. The next step is going over all of the image and taking the values your filters output. And you have several different convolutions (every convolution has its own filter). And what you get when you go over all of the picture with convolution is your convolved feature (or your convolution map, or your feature map). That black image with white edges of a deer is a convolution map. So you have set of some different maps and the next convolution layer looks at each map of the set like color channels (so it's still 2D convolution).\\
\begin{figure}[h]
  \centering
  \begin{subfigure}[l]{0.6\linewidth}
    \includegraphics[width=\linewidth]{7h.png}
    \caption*{(7.3) Convolution layers}
  \end{subfigure}
\end{figure}

\vspace{-0.5cm}
\subsubsection*{Pooling}

Another operation we use to decrease complexity is called pooling. For example, if there is a clear signal on some area of a feature map, then we just need a point with that signal because all the neighboring points probably are signaling the same. So we just take the maximum and it's called maximum pooling. Also in some cases you can take the average (and you'll have average pooling).

\subsubsection*{Padding}

Padding is one way to perform convolution. For example, we have an image of size $M\times N$ and a filter of size $(2K+1)\times(2K+1)$. Before we start moving the filter, we resize the original image to $(2K+M)\times(2K+N)$, adding a frame with weight $K$, consisting of zeros.

\section{Examples of Deep Neural Networks}
\vspace{-0.6cm}
\subsubsection*{LeNet-5}

One of the first deep learning networks. It was created in 1998 and it used all the algorithms and computer vision techniques of that time. Also it used only 61470 weights. After that network there was a deep learning winter: almost no one worked with them (except several labs across the world) until 2012.

\subsubsection*{AlexNet}

In 2012 there was a ImageNet competition when AlexNet won by a huge margin with 15\% error (in 2011 the 1st place had 27\%). This is the first deep network that was proved to be so ahead of everything else, and after that people started to do everything with deep learning.
AlexNet is a deep neural network that has 100,207,632 weights and it uses a GPU's calculations (what made this net successful in 2012). Also it has two parts, each calculated individually on its own GPU. So here are some features of this net:\\
\begin{enumerate}[label=$\bullet$]
  \item Scales all images to $256\times256$, then takes random $224\times224$ batches and mirrors them.
  \item Substracts average pixel value from each pixel.
  \item ReLU ($\max(0,x)$).
  \item Dropout 0.5.
  \item Batch size 128, SGD with momentum 0.9 (momentum is how much of the previous gradient descent steps you keep), L2 weight decay ($\lambda=0.0005$).
\end{enumerate}
The kernels on GPU 1 are largely color-agnostic, while the kernels on on GPU 2 are largely color-specific. This kind of specialization occurs during every run and is independent of any particular random weight initialization (modulo a renumbering of the GPUs) [\href{https://www.nvidia.cn/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf}{ссылка} на статью с более подобным описание для интересующихся].\\
Today we don't use AlexNet, we use one of three things: VGGNet, Inception network or ResNet.

\subsubsection*{VGGNet}

VGGNet is a very deep convolutional network (VGG is the lab which invented it). The main idea is to replace some layers of wide convolutions by more layers of $3\times3$ convolutions. It works because several layers of $3\times3$ have less weights than a big initial convolution. So it saves weights and it's very deep (but not heavy computationally). VGGNet is still used today when lacking Google resources.

\subsubsection*{Inception network}

But when we have Google resources we can do something more complex. There are two main ideas of the Inception network (named after the movie). The first is using $1\times1$, $3\times3$ and $5\times5$ convolutions in one layer and then concatenating the results. The second idea is to use $1\times1$ convolutions to made other filters flat. So there are inception modules:\\
\begin{figure}[h]
  \centering
  \begin{subfigure}[l]{0.4\linewidth}
    \includegraphics[width=\linewidth]{7i.png}
  \end{subfigure}
  \hspace{0.5cm}
  \begin{subfigure}[r]{0.4\linewidth}
    \includegraphics[width=\linewidth]{7j.png}
  \end{subfigure}
  \caption*{(7.4) Inception modules}
\end{figure}
And there are Inception network (also it named GoogLeNet):
\begin{figure}[h]
  \centering
  \begin{subfigure}[c]{0.7\linewidth}
    \includegraphics[width=\linewidth]{7k.png}
    \caption*{(7.5) GoogLeNet}
  \end{subfigure}
\end{figure}

\vspace{-0.5cm}
\subsubsection*{ResNet}

As you can see the GoogLeNet has multiple outputs. The reason is that information is lost the deeper you go --- it is nice to have information from different layers. But instead of this we can add additional information: add the output from the previous layer to the output of next layer. It is called a residual connection [pic. 7.6] (also called skip connections). Also we have a highway connection [pic. 7.7]: when you train in a separate neural network to determine how much of the output we add and how much of the previous input we keep.\\
\begin{figure}[h]
  \centering
  \begin{subfigure}[l]{0.31\linewidth}
    \includegraphics[width=\linewidth]{7l.png}
    \caption*{(7.6) Residual connection}
  \end{subfigure}
  \hspace{2cm}
  \begin{subfigure}[r]{0.4\linewidth}
    \includegraphics[width=\linewidth]{7m.png}
    \caption*{(7.7) Highway connection}
  \end{subfigure}
\end{figure}
So this idea is used in ResNet. ResNet is the most widely used deep learning network. Its error rate is only 3.57\%. However, if you want to have less weights, you'll go to VGG: it still works well (currently it has error rate 6.8\%).\\
The problem of that error rate is that human error rate is near 5\%. And error rate less than 5\% means the training is on noise, because all image datasets are created by humans. So after 2015 image networks changed from image classification to object detection.

\section{Techniques in Image Analysis}
\vspace{-0.6cm}
\subsubsection*{Image augmentation}

The existing data is almost never enough. There is some ways to make your dataset bigger:\\
\begin{enumerate}[label=$\bullet$]
  \item Flip image. Be careful, some flipped objects can be from another class: a car upside down in some cases may be trash.
  \item Rotate image. When you rotate, for example, by 45 degrees you may interpolate the result: fill the corners using a part of the initial picture [pic. 7.8] or using the edge pixels of the rotated image [pic. 7.9].\\
  \begin{figure}[h]
  \centering
  \begin{subfigure}[l]{0.2\linewidth}
    \includegraphics[width=\linewidth]{7o.png}
    \caption*{(7.8) Symmetric}
  \end{subfigure}
  \hspace{2cm}
  \begin{subfigure}[r]{0.2\linewidth}
    \includegraphics[width=\linewidth]{7n.png}
    \caption*{(7.9) Edge}
  \end{subfigure}
\end{figure}
  \item Crop image. This is very powerful because it can create a lot of images.
  \item Scale image.
  \item Use Gaussian noise. After adding noise it becomes harder to overfit.
  \item Shift the colors by some constant. A good method of color shifting is transferring to another color space and shifting after that.
\end{enumerate}

\subsubsection*{Transfer Learning and Finetuning}

If image augmentation doesn't work, you can try transfer leaning. It is the most imprortant technique you can use for image analysis. For example, someone has trained on ResNet and published everything. The weights of his neural net are just a calculation of some image features. It doesn't matter what features they are: dogs, cars etc; you can train on them. Transfer learning is training a deep network on a big dataset, for example, ImageNet, then taking the results of those calculations as features, freezing them and training your small part for your specific task on output of the frozen part. It works wonderfully for any task, because there are so many features in the frozen part.\\
If you have a lot of data you can use finetuning. Finetuning is like transfer learning, but you don't fix other network weights. And you still train (finetune) to your task. Sometimes it helps but almost always it results in overfitting, because the first part is designed for millions and millions of images.