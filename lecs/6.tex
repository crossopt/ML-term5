\chapter{Neural Networks}

{\sf An inspiration to neural network machines comes from biological neural networks. In the 1950's we already knew how it works. The main thing that allows a neuron to filter all the useless signals that happen in the brain all the time is the all-or-none law: if a signal exceeds the threshold potential, the neuron will give a complete response; otherwise, there is no response.}

\section{Perceptron}

So the perceptron works the same way. However, it can be represented as a small neural network where input neurons are the signals multiplied by some coefficients, then all results add up and if the sum is over some threshold, we get the signal $+1$, otherwise, we get $-1$. The problem with that is we can only have one layer: if you combine several layers like how it happens in the brain, you will have no good way of training the multi-layer perceptron with the simple threshold function. So the way around it is replacing the threshold function with something more complex. The first approach is a single layer neural network with the logistic function as a threshold.

\section{Single Layer Networks}

A single layer neural network with the logistic function is called logistic regression. The logistic regression is a classification algorithm, not a regression algorithm. It's just called like that.

\subsubsection*{Logistic regression}

For the binary classification (with classes $+1$ and $-1$) the math comes out very nicely; the probability of the signal is one minus the probability of the reverse signal:
$$P(y|x)=\begin{cases}
f(x)&\text{for }y=+1,\\
1-f(x)&\text{for }y=-1.
\end{cases}$$
This sigmoid function $\sigma$ satisfies both parameters of $P(x|y)$:
$$P(y|x)=\sigma(yw^Tx),\qquad\sigma(s)=\frac{1}{1+e^{-s}},\qquad \sigma(s)=1-\sigma(s)$$
where $w^Tx$ is the answer of the perceptron in input $x$.\\
So if our output is the probability of classes, how would we calculate the loss function? How we would calculate the error which we can try to minimize? Well, we just use likelihood:
$$\prod\limits_{i=1}^{N}P(y_i|x_i)=\prod\limits_{i=1}^{N}\sigma(y_iw^Tx_i)$$
Then we go from multiplication to the sum of the logarithms and get the loss function (which we will try to minimize) for the logistic regression:
$$L(w)=-\frac{1}{N}\ln\left(\prod\limits_{i=1}^N\sigma(y_iw^Tx_i)\right)=\frac{1}{N}\sum\limits_{i=1}^N\ln(1+e^{-y_iw^Tx_i})$$

Now the problem is training our network. Even for one layer of logistic regression we need to utilize gradient descent:
$$w(t+1)=w(t)-\eta\frac{\partial C(w)}{\partial w}$$
The $C(w)$ is the cost function (instead of it we can use the loss function; we typically talk about the loss function $L(w)$ (or $J(w)$) and the cost function $C(w)$). And we have the learning rate $\eta$.\\
In case of large data it becomes hard to calculate the average value of all logarithms in the loss function $L(w)$. However, we can get around this problem by using a stochastic gradient descent: just calculate the loss function for a batch (some part of the data) and then apply the gradient descent step for that batch. So if we use the stochastic gradient descent, at first we need to split our data into random batches and complete the gradient descent step for every batch. This iteration is called an epoch. The number of steps in one epoch is number of all points divided by the size of the betch ($N / N_{batch}$).
So this is the difference between the gradient descent and stochastic gradient descent:
$$\text{Gradient Descent: }w\leftarrow w-\eta\left(-\frac{1}{N}\sum\limits_{i=1}^N\frac{y_ix_i}{1+e^{y_iw^Tx_i}}\right)$$
$$\text{Stochastic Gradient Descent: }w\leftarrow w-\eta\left(-\frac{1}{N_{batch}}\sum\limits_{x_i\in batch}\frac{y_ix_i}{1+e^{y_iw^Tx_i}}\right)$$
Also we have a very stochastic gradient descent (when every bench contains only one point).\\
%$$w\leftarrow w-\eta\frac{\partial C(w^Tx_i,y_i)}{\partial w}$$
{\it <The reason why very stochastic gradient descent is not the best way>}

\section{Several Layer Networks}

So we have an algorithm for training in a one layer neural network. The way to train several layers is back-propagation. 

\subsubsection*{Back-propagation}

At first we need to define some coefficients and numbers:
\begin{enumerate}[label=$\bullet$]
  \item $w_{jk}^l$ is the weight between neuron $j$ in layer $l-1$ and neuron $k$ in layer $l$.
  \item $x_j^l$ is the outcoming signal of neuron $j$ in layer $l$ after the activation function, i.e. $x_j^l=\sigma\left(\sum w_{ij}^lx_i^{l-1}\right)$ (for the logistic regression $\sigma$ is the sigmoid function). In the first layer $x_j^1=x_j$ ($x_j$ is a feature). You can look at all $x_i^l$ in other layers at the features as well (sometimes they called feature layers).
  \item $s_j^l$ is the incoming signal to neuron $j$ in layer $l$, $s_j^l=\sum w_{ij}^lx_i^{l-1}$
\end{enumerate}
So the values which we try to optimize are the $w$'s. We want to calculate the gradient for loss functions over the $w$'s. This is the best way to calculate the partial derivative of our loss function:
$$\nabla C_{w_{ij}^l}(w)=\frac{\partial C(w)}{\partial w_{ij}^l}=\frac{\partial C(w)}{\partial s_j^l}\times\frac{\partial s_j^l}{\partial w_{ij}^l}$$
Also we have
$$\frac{\partial s_j^l}{\partial w_{ij}^l}=x_i^{l-1}$$
because $s_j^l=\sum w_{ij}^lx_i^{l-1}$. Then we calculate $\delta_j^{l-1}$ which is defined as
$$\delta_j^{l-1}=\frac{\partial C(w)}{\partial s_j^{l-1}}$$
If the loss function $f$ is differentiable, then we just calculate all partial derivatives for the last layer $L$:
$$\delta_i^L=\frac{\partial C(w)}{\partial s_i^L},\qquad C(w)=f(x^L),\qquad x_i^L:=\sigma(s_i^L)$$
[$x_i^L$ -- то, что возвращает нейрон $i$ из последнего слоя, вероятность принадлежности объекта классу $i$]. For the previous layers we can again apply the chain rule and calculate the $\delta_i^{l-1}$ using $\delta_i^l$:
$$\delta_i^{l-1}=\frac{\partial C(w)}{\partial s_i^{l-1}}=\sum\limits_{j}\frac{\partial C(w)}{\partial s_j^l}\times\frac{\partial s_j^l}{\partial x_i^{l-1}}\times\frac{\partial x_i^{l-1}}{\partial s_i^{l-1}}=\sum\limits_{j}\delta_j^l\times w_{ij}^l\times\sigma'(s_i^{l-1})$$
So the back-propagation algorithm looks like this:
\begin{enumerate}[label=\arabic*.]
  \item Initialize weights randomly (with small random numbers in $[-0.1,0.1]$).
  \item Compute the forward path: calculate all $x$ and $s$.
  \item Go backward and calculate all $\delta$.
  \item Shift all weights: $w_{ij}^l\leftarrow w_{ij}^l-\eta x_i^{l-1}\delta_j^l$ ($\eta$ is some learning rate).
  \item Go to step 2.
\end{enumerate}
The important thing is you can calculate all $\delta$ with tensors: you don't have to iterate over each coefficient, but rather you can multiply tensors. Which is good because we can push it into the GPU's which are really good in multiplying tensors.

\subsubsection*{Activation functions}

Sigmoid: 
$$\sigma(s)=\frac{1}{1+e^{-s}}$$
Tanh:
$$\sigma(s)=\frac{e^s-e^{-s}}{e^s+e^{-s}}$$
Rectified Linear Unit (ReLU):
$$\sigma(s)=\max(0,s)$$
The sigmoid finction goes from 0 to 1. If you want something that can be negative you can use the hyperbolic tangent, which goes from $-1$ to 1. And if you want something that calculates very fast and doesn't diminish your gradient when you reach big numbers you can use ReLU.

\subsubsection*{Softmax and cross-entropy}

In binary classification you use the logistic function and you have one output neuron that returns the probability of one class. In multiclass classification we utilize the Softmax function:
$$x_j^L:=\sigma(s_j^L)=\frac{e^{s_j^L}}{\sum e^{s_i^L}}$$
It is called Softmax because all last layer neurons output numbers around 0 (except one that outputs a number around 1).\\
The cross-entropy loss function is:
$$C(w)=-\sum o_i\log(x_i^L),\qquad o_i=\begin{cases}
1&\text{if }y=i,\\
0&\text{if }y\ne i.
\end{cases}$$
The fully connected network is the network where each neuron of a layer connects with each neuron of the next layer and so on.

\section{Regularization}

Networks, especially fully connected, are very prompt to overfitting. For example let's say we have a very big network and train it to recognize images. Instead of learning features of the images, for example, recognizing a cat from a car by ears and so on, big neural networks can just memorize the images: it can looks on the value of a concrete pixel. This is an overfitting -- memorising the dataset, noise training and so on. What can we do to regularize it? 

\subsubsection*{$L_2$ regularization}

When we regularize we are trying to lower the VC-dimension, lower the number of possible hypotheses. We do that by adding some constraints to the loss function. Here we add the $L_2$ constraint:
$$C_{new}(w)=C(w)+\frac{\lambda}{2N}\|w\|_2^2$$
We want the sum of the weight be the lowest possible. This is called the $L_2$ regularization or weight decay.

\subsubsection*{Early stopping}

\begin{wrapfigure}{r}{0.4\linewidth}
  \vspace{-1.3cm}
  \begin{center}
    \includegraphics[width=\linewidth]{6a.png}
  \end{center}
  \vspace{-0.3cm}
  \caption*{(6.1) Early stopping}
  \vspace{-1.5cm}
\end{wrapfigure}
Another option in regularization is catching the moment of overfitting. So this is a typical graph [pic.~6.1] which shows the gain between errors on the validation dataset and training dataset (the x axis is the number of epochs).\\
The error on the training dataset is always decreasing, but the error in the validation dataset starts to increase at some point. And at that point we want to stop (remember the weights of the network in that point).

\subsubsection*{Dropout}

Dropout is a direct way to prevent networks from memorizing the dataset. We just turn of some (50\%, for example) random neurons for every layer in every patch. The turned off neuron outputs 0. The network will try to learn general features rather than specific pathways, because each pathway can be turned off.\\
\begin{figure}[h!]
  \centering
  \begin{subfigure}[l]{0.3\linewidth}
    \includegraphics[width=\linewidth]{6b.png}
    \caption*{(6.2) Standart neural net}
  \end{subfigure}
  \hspace{2cm}
  \begin{subfigure}[r]{0.3\linewidth}
    \includegraphics[width=\linewidth]{6c.png}
    \caption*{(6.3) After applying dropout}
  \end{subfigure}
\end{figure}
When we apply this network at some task we need to turn on all the neurons.

\subsubsection*{Data augmentation}

Overfitting doesn't happen if you have a lot of data. We will discuss how to create a lot of data in the next lecture.

\section{Deep Learning Libraries}

If you don't like Python, learn to like Python because most of deep learning is done in it. However there are still some enthusiasts that work on deep learning for Java \href{http://deeplearning4j.org/}{library}.\\
Google represents \href{www.tensorflow.org}{TensorFlow} and the \href{keras.io}{Keras} (which is built on top of the first one). So if you just want to create networks simply you should use Keras. And if you want to create complex networks you should use TensorFlow.\\
The \href{torch.ch}{Torch} library easier than TensorFlow but more complex than Keras.